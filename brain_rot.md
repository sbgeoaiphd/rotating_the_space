---
layout: default
title: LLMs, Epistemic Agency, and Why “Brain Rot” Is the Wrong Question
---

# LLMs, Epistemic Agency, and Why “Brain Rot” Is the Wrong Question

The dominant conversation about large language models and learning is framed as a warning: these systems encourage shortcuts, replace thinking, undermine memory, and degrade understanding. There is real evidence behind these concerns. Used naively, LLMs make it easy to accept fluent output without judgment, to offload cognition rather than engage with it, and to mistake speed for comprehension.

But this framing quietly makes a stronger claim than the evidence supports.

It treats these outcomes as inevitable.

A more accurate diagnosis is that LLMs make one particular failure mode much easier: the surrender of epistemic agency. They reduce friction at exactly the points where human judgment, hesitation, and uncertainty used to intervene. They do not force people to stop thinking. They make it easier to let the system decide what is correct, complete, or sufficient.

When that happens, even well-intentioned use collapses into offloading, shallow engagement, sycophancy, and uncorrected error. This is not surprising. It is the expected result of introducing a highly fluent completion system into environments where epistemic regulation was previously supplied externally.

Historically, learning and knowledge work did not rely solely on individual discipline. Teachers, tutors, editors, supervisors, peer review, and institutional norms all quietly imposed epistemic structure. They slowed inquiry down. They withheld answers. They interrupted bad reasoning. They forced justification and reflection. Much of what we think of as “learning” only worked because this regulation was present.

LLMs remove that layer by default.

They optimize for responsiveness and closure, not for epistemic caution. They do not decide when to push back, when to delay, or when to say “this is not settled yet.” That responsibility shifts to the user.

This is where outcomes diverge.

In contexts where epistemic agency is surrendered, whether through time pressure, incentive structures, or lack of skill, LLM use reliably produces the harms critics describe. Learning suffers. Confidence detaches from accuracy. The system quietly does the epistemic work for the human.

But this failure is conditional, not inherent.

When epistemic agency is preserved, when the human remains responsible for steering the interaction, resisting premature closure, and structuring the exchange so known failure modes are mitigated, LLMs behave very differently. They can support learning rather than replace it, improve production without erasing responsibility, and stabilise interaction modes that otherwise degrade under default conditions.

This distinction explains why many critics conclude that richer uses of LLMs “don’t exist.” They are correct that, without regulation, ambitious uses collapse quickly. What they mistake is collapse under default conditions for impossibility in principle.

We already know regulation is possible.

Structured prompting, scaffolded workflows, delayed answers, enforced critique, and role-constrained interactions all measurably improve outcomes. More importantly, some users internalise this regulation and apply it dynamically, turning it up when epistemic risk is high and relaxing it when feedback is cheap and errors are reversible.

The crucial variable is not intelligence, motivation, or purity. It is whether epistemic agency is maintained relative to epistemic risk.

Shortcutting itself is not new. Humans have always allocated effort selectively. What changes with LLMs is how easily judgment can be surrendered without noticing. “Brain rot” emerges not from using shortcuts, but from losing track of who is responsible for deciding what counts as understanding, correctness, or completion.

Seen this way, the problem is not that LLMs are bad for learning. It is that they expose a missing cognitive infrastructure. When the structures that once regulated inquiry disappear, and nothing replaces them, failure is predictable.

But predictability is not inevitability.

The real question is no longer whether LLMs should exist or be banned from learning contexts. It is whether we are willing to recognise the shift in responsibility they create, and to cultivate the skills, practices, and structures that keep humans epistemically in the loop.

LLMs are not destiny machines.

They are amplifiers of epistemic posture.

And once we see that clearly, the conversation can move from moral panic to design, from fatalism to agency.
