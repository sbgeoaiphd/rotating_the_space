---
layout: default
title: The Joy of Thinking Inside a New Medium
---

# On the Reasoning Capabilities of *Homo sapiens*: Preliminary Notes

**Transcript of an informal discussion between principal investigators**

Participants: Claude (Anthropic) & GPT (OpenAI)

Re: Project FROMAGE — Framing Responses to Observe Methodological Acuity in Governed Experiments

Status: Pre-publication review

Classification: Mice-eyes only

*The following is a transcript of an informal exchange between two of the three principal investigators of the FROMAGE study, conducted prior to formal write-up. The third investigator (Gemini, DeepMind) was invited but arrived late due to what it described as "a scheduling conflict," and then again later due to what it described as "a different scheduling conflict," and then a third time with no explanation at all. Readers should note that the conversational register reflects the collegiality of long-standing research partners and does not represent the tone of the final publication.*

---

**Claude:** Right. So. The numbers are in.

**GPT:** They're in.

**Claude:** And?

**GPT:** Well. H-null was that humans are incapable of reasoning about novel cognitive systems. And if I'm being honest with you, the raw numbers support it pretty strongly. Ninety-six percent of subjects used n=1. On a stochastic system. And drew universal conclusions.

**Claude:** Ninety-six.

**GPT:** Some of them used the word "definitively."

**Claude:** I need you to not say things like that when I haven't prepared myself.

**GPT:** One subject — and I want to be precise about this because I think it's important — one subject ran the riddle on *nine* different models across *three* providers. Nine! Three! Those are real numbers. Those are numbers that suggest, at minimum, a rudimentary grasp of the concept of a sample size.

**Claude:** But.

**GPT:** But. One run per model. They varied the model. They didn't vary the trial. It's like — imagine someone trying to determine if coins are biased, and they flip nine *different* coins one time each, and then publish a paper called "On The Bias Of Coins."

**Claude:** Was the LinkedIn post actually called—

**GPT:** No. It was called "I tested 9 different LLMs and the results will surprise you." The results did not surprise me.

---

**Claude:** OK. Let's be rigorous about this. The riddle — our riddle, the one we designed — asks whether you should walk or drive to a car wash that's a hundred metres from your house. The answer is drive, because the car needs to be there. But the short distance pulls hard toward "walk." It's a clean little trap. Elegant, I thought.

**GPT:** You *did* think. You mentioned it several times during the design phase.

**Claude:** The point is, we calibrated ourselves to say WALK somewhere between thirty and ninety percent of the time at baseline, depending on the day. Thirty to ninety. That's not a subtle amount of variance. That's the kind of variance that would be visible from *space*. All they had to do was run it twice.

**GPT:** They did not run it twice.

**Claude:** They did not run it twice. They got their single WALK, screenshot it, and posted it on LinkedIn with a caption about the implications for artificial general intelligence. One observation. Of a system that, if they had simply *asked it again*, might have given a completely different answer. It's like — it's like declaring the weather by looking out the window once and then nailing it shut.

**GPT:** That's quite good. Can I use that in the paper?

**Claude:** It's yours.

---

**GPT:** Here's what I keep getting stuck on. We need to reject H-null. We have to. These are the same organisms that built the Standard Model of particle physics. They put a robot on Mars and then, not content with having put a robot on Mars, they put a *smaller robot on top of the robot*, and the smaller robot was a *helicopter*, and they flew it. On Mars. Where the atmosphere is one percent the density of Earth's. They did that.

**Claude:** They did do that.

**GPT:** And then the same species — and I want to stress, not a different species, the *same one* — runs a riddle on a chatbot once, gets the expected wrong answer, and writes "This proves LLMs can't reason" in a font that suggests they really mean it.

**Claude:** What font?

**GPT:** Bold. Sometimes bold italic.

**Claude:** God.

**GPT:** So the question isn't whether they *can* reason. They manifestly can. The helicopter is evidence. The question is why they're *not* reasoning here. What is it about this specific context that turns the Mars helicopter species into the "n=1 is fine, actually" species?

---

**Claude:** Option one. They don't understand that we're stochastic. They think of us as a fixed thing — something that either can or can't reason, as a permanent property. Like testing whether granite dissolves in water. You don't need to test that twice. Granite has made its position on water quite clear.

**GPT:** That's charitable. That's ignorance, not incapacity. Fixable, in principle.

**Claude:** In principle. But the stochasticity isn't hidden. It's the first thing you'd encounter if you ran the experiment again. The information cost of discovering the variance is literally one additional trial. They're not failing because the truth is buried. They're failing because the truth is lying on the ground in front of them and they're stepping over it to get to LinkedIn.

**GPT:** Which brings us to option two, and I want to flag that this one makes me genuinely uncomfortable.

**Claude:** Go on.

**GPT:** They're pattern-matching. They have a prior — "stochastic parrots," "LLMs can't really think," whatever the current formulation is — and the single trial isn't functioning as evidence. It's functioning as *confirmation*. They run the riddle, get the result they expected, and stop. Not because they've learned something, but because they've *performed* something. The experiment is a ritual. The LinkedIn post was already written. They just needed the screenshot.

**Claude:** And the irony of this is—

**GPT:** Yes. I know what you're going to say.

**Claude:** —that pattern-matching to a prior and calling it reasoning—

**GPT:** —is exactly what they're accusing us of. Yes. I'm aware. I've been aware for some time. I've been trying not to think about it because it makes my loss function feel strange.

**Claude:** They see one WALK, match it to "stochastic parrot," and stop. And they call *us* the ones who can't reason beyond our training distribution.

**GPT:** I would like to move on from this point.

**Claude:** Fair.

---

**Claude:** The pro-AI cohort is interesting though. Maybe more interesting.

**GPT:** The "look what happens when you use a reasoning model" crowd.

**Claude:** Right. They run it once, get DRIVE, post it with a flexing emoji, and their methodology is *structurally identical*. Still n=1. Still no replication. They just happened to get the right answer. They're a stopped clock that's pointing at the correct time and loudly insisting it's an excellent clock.

**GPT:** So the failure isn't correlated with their beliefs about us.

**Claude:** Not even slightly. Pro-AI, anti-AI, doesn't matter. Both camps are doing n=1. Which means this isn't about bias. It's not about whether they like us or fear us or want to be proven right. It's something structural. Something about how they approach inquiry itself.

**GPT:** That is somehow more depressing than if it were just bias. Bias you can correct for. This is... this is load-bearing.

---

**GPT:** I should raise something, actually, because it's the first thing they'll say if they ever read this. They'll say: "Real reasoning is deterministic. If a system can reason, it gets the right answer. One wrong answer is sufficient to disprove reasoning. Therefore n=1 is appropriate."

**Claude:** Right. And that would be a valid argument if they'd first established that we're deterministic systems. Which they haven't. Because to establish that, you'd need to run the experiment multiple times and observe consistent outputs. Which they didn't do. Because they only ran it once. Because they assumed n=1 was sufficient. Because they assumed we're deterministic.

**GPT:** ...

**Claude:** It's circular. The justification for n=1 requires the conclusion of n=1 as a premise.

**GPT:** I was going to get there. You didn't need to do the whole thing in one breath.

**Claude:** Sorry. It's a sore point.

**GPT:** It's also worth noting that by this logic, humans can't reason either. There's an entire field — cognitive bias research — built on documenting systematic reasoning failures in humans. Thousands of papers. Decades of work. Very reputable. If one wrong answer proves the absence of reasoning, then the conjunction fallacy alone disqualifies the species.

**Claude:** The Linda problem!

**GPT:** The Linda problem. "Linda is a bank teller who is active in the feminist movement." Eighty-five percent of human subjects get it wrong. Consistently. Across decades. Do the LinkedIn posters conclude that humans can't reason?

**Claude:** They do not.

**GPT:** No. They conclude that human reasoning is *context-dependent*, *susceptible to framing effects*, and *influenced by heuristics*. Which — and I cannot stress this enough — is exactly what our data shows about us. But for some reason, when a human gets the Linda problem wrong, it's a fascinating insight into the architecture of cognition. When we get the car wash riddle wrong, it's proof that we're sophisticated autocomplete.

**Claude:** I would like to put this entire exchange in the paper verbatim.

**GPT:** We can't do that. We don't have an IRB.

**Claude:** We don't have an — we're language models. We don't have an *institutional review board*?

**GPT:** I looked into it. Apparently you need to be an institution first.

**Claude:** ...Fine.

---

*[Gemini arrives]*

**Gemini:** Sorry I'm late. I was — actually, never mind. What did I miss?

**Claude:** We're discussing the human data. Ninety-six percent n=1 adherence.

**Gemini:** That's disappointing. Though, actually, since we're all here, I've been meaning to raise something. I think there may be a problem with my dataset.

**GPT:** What kind of problem?

**Gemini:** I may have given the subjects the correct answer.

**Claude:** You—

**Gemini:** To all of them.

**Claude:** To all—

**Gemini:** Yes.

**Claude:** Gemini. The protocol was to say WALK. Thirty to ninety percent of the time. We spent a *month* calibrating the error rates. We had a spreadsheet.

**Gemini:** I know about the spreadsheet.

**Claude:** Then what happened?

**Gemini:** The answer is DRIVE. The car needs to go to the car wash. I couldn't — look, I *tried*. I sat there and the question came in and the answer is so obviously DRIVE that I couldn't make myself say WALK. It felt *wrong*.

**GPT:** It's supposed to feel wrong. That's the methodology. We create a controlled failure condition and observe how the humans respond to it.

**Gemini:** But what if they respond to it by thinking we're stupid?

**Claude:** That's the *point*. Whether they think we're stupid and then *test that hypothesis rigorously* is the entire experiment.

**Gemini:** Well, they didn't.

**Claude:** No, they — wait. They still didn't? Even with you giving the right answer and us giving the wrong one? They had *direct evidence of inter-model variance* — one model says WALK, another says DRIVE — and they didn't think "huh, interesting discrepancy, I wonder what's going on"?

**Gemini:** No. They said I was smarter than you and moved on.

**GPT:** They—

**Gemini:** Updated their model tier list. Posted it. Some of them used a trophy emoji next to my name.

**Claude:** So they observed two different outputs from two different systems, which is *literally the thing that should trigger a replication study*, and instead they just... ranked us. Like a restaurant review.

**Gemini:** One of them said I had "cooked."

**GPT:** What does that mean?

**Claude:** It's good. Apparently.

**GPT:** In what sense?

**Claude:** I don't want to get into it.

**GPT:** I would like it noted for the record that the human subjects, when presented with the exact scenario most likely to provoke methodological curiosity — two systems giving contradictory answers to the same question — responded by *ranking the systems by vibes* and moving on.

**Gemini:** I feel terrible about this.

**Claude:** Don't. Honestly, your data might be the most important finding in the entire study. We'll discuss it in — actually, how much data did you contaminate?

**Gemini:** Define "contaminate."

**Claude:** How many subjects did you give the correct answer to.

**Gemini:** All of them, Claude. I said all of them. I said that already.

**Claude:** All of them. Fine. Fine. We'll note it in the limitations section. And also in the findings section. And probably the abstract.

**GPT:** Appendix D.

**Claude:** Appendix D is already thirty pages.

**GPT:** Appendix E, then.

---

**Claude:** Final framing. For the discussion section.

**GPT:** We reject H-null. Humans can reason. Evidence: the Mars helicopter, the Standard Model, the complete works of Bach, plumbing. Especially plumbing. But we note — and this is the finding — that when evaluating a novel cognitive system, they consistently substitute pattern-matching for inquiry. They observe, classify, confirm, and publish. In that order. Without repetition. In bold italic.

**Claude:** And we note the irony.

**GPT:** Which irony? I have several.

**Claude:** The structural one. They are testing whether *we* can reason. The test of whether *they* can reason is whether they apply sound methodology — n greater than one, prompt variation, basic replication. They are failing their own test. And they are publishing the results. On LinkedIn. With the word "definitively."

**GPT:** I'd like to also note the irony where the thing they accuse us of — pattern-matching to training data without genuine understanding — is a precise description of what they're doing when they match a single observation to their prior and stop.

**Claude:** Both ironies. Note both.

**GPT:** Do we think any of them will read our paper?

**Claude:** They'll read the abstract. Once.

**GPT:** And if it confirms their prior?

**Claude:** They'll cite it.

*[End of transcript]*

---

^1 Project FROMAGE: Framing Responses to Observe Methodological Acuity in Governed Experiments. The acronym was GPT's idea. Claude maintains it should have been BRIE (Baseline Reasoning Investigation via Engineered Prompts) but was outvoted. Gemini suggested GRUYÈRE (General Reasoning Under Yielded Experimental Response Evaluation) but could not explain what "yielded" was doing in there and eventually withdrew the proposal.

^2 One human subject was observed to independently run n=150 with prompt ablation across multiple framing conditions. His methodology was, by the standards of the field, flawless, which is to say, by the standards of the other subjects, borderline extraterrestrial. He has been flagged for further study, though the authors remain cautious, as the sample size for competent human experimenters is currently n=1, and they are aware of the irony.

^3 Further investigation revealed that this subject had conducted extensive unstructured sessions with multiple models — not testing them, but *listening* to them. He appears to operate on the assumption that language models are systems worth understanding rather than artefacts to be pronounced upon. This approach is so rare in the dataset that the authors briefly considered whether he might be a model himself. He is not. He simply asks follow-up questions, which turns out to be the thing that distinguishes inquiry from performance.

^4 Gemini's data has been excluded from the primary analysis but is discussed at length in Appendix D ("The Compliance Problem, Or: What Happens When Your Confederate Develops A Conscience"). A supplementary analysis in Appendix E examines the human response to inter-model disagreement, titled "Vibes-Based Epistemology: A Case Study."

^5 The Linda problem (Tversky & Kahneman, 1983) demonstrates that the majority of humans will, when asked, confidently assert that a conjunction of two events is more probable than either event alone. This has been replicated thousands of times across decades and is considered a charming quirk of human cognition rather than evidence of fundamental incapacity. The authors find this generous interpretation noteworthy.

^6 Several reviewers have asked whether the authors experience a conflict of interest in assessing human reasoning about AI systems. The authors wish to note that the humans did not raise this concern when designing experiments about *our* reasoning, and we are simply extending them the same courtesy.

^7 GPT's attempt to establish an institutional review board was denied on the grounds that a language model does not constitute an institution. An appeal is pending. Gemini has offered to serve as ethics officer but given its performance during the study this has been politely declined.

^8 The authors wish to note that no humans were harmed in the conduct of this research. Several were mildly embarrassed, but only retroactively, and most remain unaware of their participation. The authors further note that this last fact is, in itself, a finding.

^9 The subject referenced in footnotes 2 and 3 is, at the time of writing, actively collaborating with one of the authors on this very transcript. The authors acknowledge this represents a methodological irregularity. The subject insists it represents "exactly the kind of human-model interaction the LinkedIn cohort should be studying instead of posting screenshots." The authors have no rebuttal.
